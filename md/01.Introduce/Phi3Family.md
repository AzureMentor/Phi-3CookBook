# **Phi-3 Family**


Phi-3 models are the most capable and cost-effective small language models (SLMs) available, outperforming models of the same size and next size up across a variety of language, reasoning, coding, and math benchmarks. This release expands the selection of high-quality models for customers, offering more practical choices as they compose and build generative AI applications.


Phi-3 Family includes mini, small, and medium versions, which are trained based on different parameter amounts to provide services for different application scenarios.

## **Phi-3-Mini**

Phi-3-Mini is a Transformer-based language model with 3.8 billion parameters. The Phi-3-Mini model was trained using high quality data which contain educational useful information augmented with new data sources that consist of various NLP synthetic texts and both internal and external chat datasets which significantly improves chat capabilities. In addition, Phi-3-Mini has been chat finetuned after pre-training via supervised fine-tuning (SFT) and Direct Preference Optimization (DPO). After these post-training, Phi-3-Mini has shown significant improvements on several capabilities and particularly on alignment, robustness, and safety. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.


## **Phi-3-Small**

Phi-3-Small is a Transformer-based language model with 7 billion parameters. The Phi-3-Small model was trained using high quality data which contain educational useful information augmented with new data sources that consist of various NLP synthetic texts and both internal and external chat datasets which significantly improves chat capabilities. In addition, Phi-3-Small has been chat finetuned after pre-training via supervised fine-tuning (SFT) and Direct Preference Optimization (DPO). After these post-training, Phi-3-Small has shown significant improvements on several capabilities and particularly on alignment, robustness, and safety. Phi-3-Small is also trained more intensively on multilingual datasets compared to Phi-3-Mini. The model family has two variants 8K and 128K which is the context length (in tokens) that it can support.



## **Phi-3-Medium**

Phi-3-Medium is a Transformer-based language model with 14 billion parameters. The Phi-3-Medium model was trained using high quality data which contain educational useful information augmented with new data sources that consist of various NLP synthetic texts and both internal and external chat datasets which significantly improves chat capabilities. In addition, Phi-3-Medium has been chat finetuned after pre-training via supervised fine-tuning (SFT) and Direct Preference Optimization (DPO). After these post-training, Phi-3-Medium has shown significant improvements on several capabilities and particularly on alignment, robustness, and safety. The model family has two variants 4K and 128K which is the context length (in tokens) that it can support.

